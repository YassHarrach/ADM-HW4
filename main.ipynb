{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1baf70e9",
   "metadata": {},
   "source": [
    "# 2. Grouping customers together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1cee6",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea20247",
   "metadata": {},
   "source": [
    "We need to pre_process our data to delete all the outliers, NA or mistakes and set the time variables in a right form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import sklearn.metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1ff089",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = pd.read_csv(\"bank_transactions.csv\")\n",
    "bank_data.dropna(inplace=True)\n",
    "bank_data.CustomerDOB = pd.to_datetime(bank_data.CustomerDOB)\n",
    "bank_data.TransactionDate = pd.to_datetime(bank_data.TransactionDate)\n",
    "bank_data.TransactionTime = bank_data.TransactionTime.apply(lambda x: datetime.strptime(str(x).zfill(6), \"%H%M%S\"))\n",
    "bank_data = bank_data.rename(columns={'TransactionAmount (INR)':'TransactionAmount'})\n",
    "bank_data[\"TransactionTime\"] = bank_data[\"TransactionTime\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70251fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.CustomerDOB.dt.year.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691ee11",
   "metadata": {},
   "source": [
    "Watching the histogram we can see that there are a lot of people born in 1800 and others in 2050 and after. The last range is a mistake for the millenium bug, so we can fix it by subtracting 100 years in each of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c86beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.drop(bank_data[bank_data.CustomerDOB.dt.year == 1800].index, axis=0, inplace=True)\n",
    "bank_data.loc[bank_data.CustomerDOB.dt.year > 2000, 'CustomerDOB'] = bank_data.loc[bank_data.CustomerDOB.dt.year > 2000, 'CustomerDOB'] - pd.DateOffset(years = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ca90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.CustomerDOB.dt.year.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a76205",
   "metadata": {},
   "source": [
    "Now the histogram shows right values of birthdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.to_pickle(\"bankdata\")\n",
    "data = pd.read_pickle(\"bankdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e728104",
   "metadata": {},
   "source": [
    "## 2.1 Getting your data + feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558e672",
   "metadata": {},
   "source": [
    "#### Define a new index\n",
    "Now our purpose is to group by the transaction for each costumer and add new variables to create a dataset of 27 features.\n",
    "Since we discovered that the CustomerID field is not a good approximation of the customer identity, a combination of fields is being used to identify the user. We will perform the operation of grouping by using newly defined columns which we believe may represent a single customer. In the following cells two strategies are explored, in the first one the location in which a single customer does the transaction is assumed to be not changing, in the second one the balance of a single customer is considered consant. The combination of one of these two fields, along with DOB and gender will be the new identification column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262016a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New columns\n",
    "data[\"NewID1\"] = data[\"CustomerDOB\"].astype(str) + data[\"CustGender\"] + data[\"CustLocation\"]\n",
    "data[\"NewID2\"] = data[\"CustomerDOB\"].astype(str) + data[\"CustGender\"] + data[\"CustAccountBalance\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc89487",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"CustomerID\", \"NewID1\", \"CustAccountBalance\", \"TransactionAmount\"]].groupby([\"NewID1\"]).agg(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"CustomerID\", \"NewID2\", \"CustLocation\", \"TransactionAmount\"]].groupby([\"NewID2\"]).agg(lambda x: x.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc43a59",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "As we can see, we get a list of equal balances when we cosider the location constant and equal locations when considering the balance constant, also worth noting that the CustomerIDs are indeed different. Noting finally that the size of the two grouped by datasets is similar, we can conclude that these two operations yield similar results, corroborating the idea that these new definitions for the customer identity are more refined than CustomerID. One final note is that the majority of customers has multiple associated CustomerIDs. We now proceed to making the dataset, the new index will be the combination of DOB, gender and balance, as it yields slightly more rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Trans_over_100'] =  data.loc[data['TransactionAmount']>100,'TransactionAmount']\n",
    "data['Small_transaction'] = data.loc[data['TransactionAmount']<100,'TransactionAmount']\n",
    "data['Perc_transaction_balance'] = (data['TransactionAmount']/(data['CustAccountBalance']))*100\n",
    "data['Big_balance'] = data.loc[data['CustAccountBalance']>100000,'CustAccountBalance']\n",
    "data['Utilisation'] = data['CustAccountBalance']-data['TransactionAmount']\n",
    "data['CustomerAge'] = ((pd.to_datetime('today') - data.CustomerDOB) / np.timedelta64(1,'Y')).astype(int)\n",
    "data['Day_week'] = data.TransactionDate.dt.day_of_week\n",
    "data['Month'] = data.TransactionDate.dt.month\n",
    "data['Summer'] = data.loc[(data.TransactionDate.dt.month>=6) & (data.TransactionDate.dt.month<=9),'TransactionAmount']\n",
    "data['Winter'] = data.loc[(data.TransactionDate.dt.month>=12) | (data.TransactionDate.dt.month<=2),'TransactionAmount']\n",
    "data['Begin_month'] = data.loc[data.TransactionDate.dt.day<=15,'TransactionAmount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346eebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe={\n",
    "           'Transactions_#':list(data.groupby(\"NewID2\").TransactionID.count()), # Number of transactions\n",
    "           'Ts_over_100':list(data.groupby(\"NewID2\").Trans_over_100.count()), # Number of transactions over 100 units of currency\n",
    "           'Small_transactions_#':list(data.groupby(\"NewID2\").Small_transaction.count()), # Number of transactions under 100 units of currency\n",
    "           'Ave_transaction':list(data.groupby(\"NewID2\").TransactionAmount.mean()), # The average transaction\n",
    "           'Min_transaction':list(data.groupby(\"NewID2\").TransactionAmount.min()), # The minimum transaction\n",
    "           'Max_transaction':list(data.groupby(\"NewID2\").TransactionAmount.max()), # The maximum transaction\n",
    "           'Balance':list(data.groupby(\"NewID2\").CustAccountBalance.first()), # The average balance\n",
    "           'Mean_%_Tr_balance':list(round((data.groupby(\"NewID2\").Perc_transaction_balance.mean()),1)), # The percentage of the transactions on the balance\n",
    "           'Average_util':list(data.groupby(\"NewID2\").Utilisation.mean()), # The average utilisation\n",
    "           'Gender':list(data.groupby(\"NewID2\").CustGender.first()), # The gender most common\n",
    "           'Location':list(data.groupby(\"NewID2\").CustLocation.agg(lambda x: pd.Series.mode(x)[0])), # The location most common\n",
    "           'Age':list(data.groupby(\"NewID2\").CustomerAge.first().astype(int)), # The average of the ages\n",
    "           'Big_balance':list(data.groupby(\"NewID2\").Big_balance.count()), # A boolean for people under (0) or over (1) 100000 in balance\n",
    "           'Day_week':list(data.groupby(\"NewID2\").Day_week.agg(lambda x: pd.Series.mode(x)[0])), # The most common day of the week\n",
    "           'Month':list(data.groupby(\"NewID2\").Month.agg(lambda x: pd.Series.mode(x)[0])), # The most common month\n",
    "           'Summer':list(data.groupby(\"NewID2\").Summer.sum()), # The amount of transactions during summer\n",
    "           'Winter':list(data.groupby(\"NewID2\").Winter.sum()), # The amount of transactions during winter\n",
    "           'Begin_month':list(data.groupby(\"NewID2\").Begin_month.sum()), # The amount of transactions during the first half of all months\n",
    "           \"Accounts_#\":list(data.groupby(\"NewID2\").CustomerID.count()), # The number of accounts that have each customer\n",
    "           \"Total_expenses\":list(data.groupby(\"NewID2\").TransactionAmount.sum()), # The sum of all transactions of a customer\n",
    "           \"1st_quart_Tr\":list(data.groupby(\"NewID2\").TransactionAmount.quantile(0.25)), # First quartile of the transactions amount\n",
    "           \"3rd_quart_Tr\":list(data.groupby(\"NewID2\").TransactionAmount.quantile(0.75)), # Third quartile of the transactions amount\n",
    "           \"Median_Tr\":list(data.groupby(\"NewID2\").TransactionAmount.median()), # Median of the transactions amount\n",
    "           \"Hour\":list(data.groupby(\"NewID2\").TransactionTime.agg(lambda x: pd.Series.mode(x)[0])), # Most common hour of the day\n",
    "           \"Median_util\":list(data.groupby(\"NewID2\").Utilisation.median()), # Median of utilisation\n",
    "           \"1st_quart_util\":list(data.groupby(\"NewID2\").Utilisation.quantile(0.25)), # First quartile of utilisation\n",
    "           \"3rd_quart_util\":list(data.groupby(\"NewID2\").Utilisation.quantile(0.75)) # Third quartile of utilisation\n",
    "           \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dataframe)\n",
    "df.to_pickle(\"2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb032045",
   "metadata": {},
   "source": [
    "## 2.2 Choose your features (variables)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab39ca",
   "metadata": {},
   "source": [
    "In order to do the Principal Component Analysis we want to take only numerical variables and normalize them. This is done for a better comparison between variables that have completely different values or unit of measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Transactions_#','Ts_over_100','Small_transactions_#','Ave_transaction','Min_transaction','Max_transaction','Balance','Average_util','Age','Big_balance','Day_week','Month','Summer','Begin_month',\"Accounts_#\",\"Total_expenses\",\"1st_quart_Tr\",\"3rd_quart_Tr\",\"Median_Tr\",\"Median_util\",\"1st_quart_util\",\"3rd_quart_util\"]\n",
    "\n",
    "x = df.loc[:, features].values\n",
    "\n",
    "# Standardizing and normalizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "z = normalize(x)\n",
    "newdf=pd.DataFrame(z, columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3bd8b",
   "metadata": {},
   "source": [
    "Now that we have our dataset rescaled and normalized, we can use the method of the Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA\n",
    "pca = PCA()\n",
    "# Determine transformed features\n",
    "z = pca.fit_transform(z)\n",
    "# Determine explained variances and the cumulative sum\n",
    "exp_var_pca = pca.explained_variance_ratio_\n",
    "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "# Plot\n",
    "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5604ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cum_sum_eigenvalues[cum_sum_eigenvalues<0.70])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3a977",
   "metadata": {},
   "source": [
    "We can notice that the first 4 principal components explain more than 70% of variance so now we want to create a dataframe with only these 4 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=4)\n",
    "PCA_f = pca1.fit_transform(newdf)\n",
    "PCA_DF = pd.DataFrame(data = PCA_f, columns = ['pc1', 'pc2','pc3', 'pc4'])\n",
    "PCA_DF.to_pickle(\"pcadf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab826684",
   "metadata": {},
   "source": [
    "## 2.3 Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d70ea0",
   "metadata": {},
   "source": [
    "#### MapReduce in pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"pcadf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a910ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark initialization\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"pcadf\")\n",
    "values = df.values.tolist()\n",
    "sdf = spark.createDataFrame(values,[\"pc1\",\"pc2\",\"pc3\",\"pc4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdc111",
   "metadata": {},
   "source": [
    "In the following cell is presented the code for the kmeans algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36925ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "rdd = sdf.rdd.map(list)\n",
    "vectorized_rdd = rdd.map(lambda row: np.array(row))\n",
    "\n",
    "def distance(x, y):\n",
    "    distance = np.zeros(len(y))\n",
    "    for i in range(len(y)):         \n",
    "        distance[i] = np.linalg.norm(x-y[i])\n",
    "    key = np.argmin(distance)\n",
    "    return (key, x)\n",
    "\n",
    "init_c = np.array(df.sample(k))\n",
    "\n",
    "diff = 999\n",
    "while diff > 0.01:  # threshold for shortening computation time\n",
    "    d = vectorized_rdd.map(lambda row: distance(row,init_c))\n",
    "    updated_c = d.groupByKey().mapValues(lambda row: sum(row)/len(row)).collect()\n",
    "    \n",
    "    K_new = {}                               #\n",
    "    for i in updated_c:                      # \n",
    "        K_new[i[0]]=i[1]                     # this block of code exists to keep\n",
    "    finalist = []                            # track of the index of each centroid\n",
    "    for i in range(k):                       #\n",
    "        finalist.append(np.array(K_new[i]))  #\n",
    "    final_c = np.array(finalist)\n",
    "    \n",
    "    diff = np.linalg.norm(final_c-init_c)\n",
    "    init_c = final_c\n",
    "    print(diff)  # printing out the difference value to monitor the execution of the algorithm\n",
    "    \n",
    "print(init_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f05d8",
   "metadata": {},
   "source": [
    "#### Optimal clustering\n",
    "We will use the methods of the elbow and the silhouette to find out what is the optimal number of clusters in terms of scoring function and overlap. In this section will be used the already implemented version of kmeans inside sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c454090",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "for k in range(1,10):\n",
    "    kmeans = KMeans(n_clusters=k).fit(df)\n",
    "    model[k] = kmeans.inertia_\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(list(model.keys()), list(model.values()))\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e689e26",
   "metadata": {},
   "source": [
    "The elbow method gives as optimal numbers of clusters a range between 4 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d2894",
   "metadata": {},
   "source": [
    "Silhouette scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ea2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,10):\n",
    "    labels = KMeans(n_clusters=i).fit(df).labels_\n",
    "    print(str(i)+\"==\"+str(sklearn.metrics.silhouette_score(df,labels,sample_size=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af55a1",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "As we can see from the values of the silhouette scores, the clusters could have a bit of overlap. Nevertheless we can conclude that in this case the optimal number of clusters will be 5 as it yields the highest silhouette score and it is in line with the results of the elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be925cc",
   "metadata": {},
   "source": [
    "Comparing our kmeans algorithm with the version of kmeans++ implemented in the sklearn package, it can be seen that the resulting centroids are very similar vecors (the order is shuffled), this discrepancy is due to the threshold we opted to use in our version for time saving reasons. This last consideration points out the main difference between the sklearn version and ours, which is the running time, as the former is way faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, init='k-means++').fit(df)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63b054",
   "metadata": {},
   "source": [
    "## 2.4 Analysing your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bbe8e",
   "metadata": {},
   "source": [
    "We created a new column that shows the cluster to which each row belongs. Then we choose the average transaction, balance and total expences as more relevant variables relevant to identify the cluster of each customer. With the three functions below we want to divide in 4 classes each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06336f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, init='k-means++')\n",
    "df[\"cluster\"] = kmeans.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd264a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogdf = pd.read_pickle(\"2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classTransactions(x):\n",
    "    if x < 1000: return(\"0-1000\")\n",
    "    elif x <2500: return(\"1000-2500\")\n",
    "    elif x < 4000: return(\"2500-4000\")\n",
    "    else: return(\">4000\")\n",
    "        \n",
    "def classBalance(x):\n",
    "    if x < 10000: return(\"0-10000\")\n",
    "    elif x <50000: return(\"10000-50000\")\n",
    "    elif x < 200000: return(\"50000-200000\")\n",
    "    else: return(\">200000\")\n",
    "    \n",
    "def classExpenses(x):\n",
    "    if x < 1000: return(\"0-1000\")\n",
    "    elif x <5000: return(\"1000-5000\")\n",
    "    elif x < 20000: return(\"5000-20000\")\n",
    "    else: return(\">20000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogdf[\"cluster\"] = df[\"cluster\"]\n",
    "ogdf[\"Transactions_class\"] = ogdf.Ave_transaction.apply(lambda row: classTransactions(row))\n",
    "ogdf[\"Balance_class\"] = ogdf.Balance.apply(lambda row: classBalance(row))\n",
    "ogdf[\"Expenses_class\"] = ogdf.Total_expenses.apply(lambda row: classExpenses(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90ed4d",
   "metadata": {},
   "source": [
    "Now that we have 3 new columns to show the class to which each row belongs, we want to create for each variable a pivot table normalized for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14475ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot1 = round(pd.crosstab(index=ogdf.cluster, columns=ogdf.Transactions_class, values=ogdf.Ave_transaction, aggfunc=\"count\", normalize=\"index\")*100,1)\n",
    "pivot2 = round(pd.crosstab(index=ogdf.cluster, columns=ogdf.Balance_class, values=ogdf.Balance, aggfunc=\"count\", normalize=\"index\")*100,1)\n",
    "pivot3 = round(pd.crosstab(index=ogdf.cluster, columns=ogdf.Expenses_class, values=ogdf.Total_expenses, aggfunc=\"count\", normalize=\"index\")*100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68da973",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773560c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e6ac5",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "1. Transactions: Clusters from 1 to 4 have a similar trend regarding the average transaction amount, in particular we see that the majority of the transactions belong to the lower ranges, and cluster 3 has a more extreme behaviour with respect to the other three.\n",
    "\n",
    "2. Balance: Also in this case the first cluster has a different trend from the others, having all four classes with similar percentages. In cluster 1 and even more in clusters 2, 3 and 4 the percentage about the lower class increases a lot, reaching values above 50%. Going through the other classes we can notice that clusters 2, 3 and 4 have a similar trend with low values for balances above 200000.\n",
    "\n",
    "3. Expenses: In the case of the total expenses, we can see that clusters 3 and 4 have a very similar trend of percentages across the classes. Cluster 1 has peaks in the higher classes, having also a very low percentage of small expenses. Cluster 0 exhibits a more balanced trend across the classes, hinting to a gaussian distribution.\n",
    "\n",
    "Summing up these observations we can conclude that Cluster 0 is composed by the wealthier people, Cluster 1 contains people more prone to spend money. Clusters 2, 3 and 4 are more representative of the behaviour of an average consumer, with Cluster 2 deviating a bit towards the behaviour of the upper clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02712d87",
   "metadata": {},
   "source": [
    "Silhouette score evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5691e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(n_clusters=5).fit(df).labels_\n",
    "print(str(sklearn.metrics.silhouette_score(df,labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616f249",
   "metadata": {},
   "source": [
    "This silhouette score indicates a bit of overlap between the clusters, as expected after seeing the behaviours of the pivot tables and from the rough evaluation in point 2.3. The clustering performance in our opinion is overall acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f1c46",
   "metadata": {},
   "source": [
    "## Algorithmic Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d7bb2",
   "metadata": {},
   "source": [
    "The idea behind this algorithm is to tranform the time interval between opening and closure $t$ of an entrance into a segment of length $t$, doing so the maximum number of superpositions between these segments for each unit of time will be the minimum number of guards required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = list(input().split())\n",
    "M = list(map(int, input().split()))\n",
    "doors = set(M)\n",
    "array = np.array(M)\n",
    "\n",
    "# transformation\n",
    "D = {}\n",
    "for i in doors:\n",
    "    temp = np.where(array==i)[0]\n",
    "    D[i] = set(range(min(temp),max(temp)+1))\n",
    "    \n",
    "# count of intersections\n",
    "count = []\n",
    "for i in range(len(array)):\n",
    "    c=0\n",
    "    for j in D.keys():\n",
    "        if i in D[j]: c+=1\n",
    "    count.append(c)\n",
    "\n",
    "# output\n",
    "if int(input_list[2])<max(count): print(\"NO\")\n",
    "else: print(\"YES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
